# -*- coding: utf-8 -*-
"""Mushroom_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ugRaFIFT0uJoPH2qQuCQ9XDLdtRBsQ-_

#Data Loading
"""

import pandas as pd

df=pd.read_csv("/content/mushrooms.csv")

print(df.head())
print(df.shape)

"""# Pre-Processing"""

print("\nMissing Values:")
print(df.isna().sum())

categorical_col = df.select_dtypes(include=['object']).columns
print("categorical_columns : ",categorical_col)

from sklearn.preprocessing import OrdinalEncoder
encoder = OrdinalEncoder()
df_encoded = encoder.fit_transform(df)
df= pd.DataFrame(df_encoded, columns=df.columns)
df.head()

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='class', data=df)
plt.title('Class Distribution')
plt.show()
conts=df['class'].value_counts()
print(conts)

"""EDA"""

import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

print("First 5 rows of the dataset:")
print(df.head())

print("\nDataset Information:")
print(df.info())

print("\nDescribing data:")
print(df.describe())

print("Descriptive statistics of the dataset:")
print(df.describe())

duplicates = df.duplicated().sum()
print(f"\nNumber of duplicate rows: {duplicates}")

numeric_col = df.select_dtypes(include=['number']).columns
print("numeric_columns : ",numeric_col)

plt.figure(figsize=(15, 10))
for i, column in enumerate(numeric_col, 1):
    plt.subplot(5, 5, i)
    sns.histplot(df[column], kde=True)
    plt.title(f'Distribution of {column}')

plt.tight_layout()
plt.show()

# Correlation matrix
corr_matrix = df.corr()

# Plot heatmap of correlation matrix
plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Features')
plt.show()

# Identifying outliers using the IQR method
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
outlier_mask = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))

# Percentage of outliers
outliers = outlier_mask.sum().sum() / np.product(df.shape) * 100
print(f"Percentage of outliers: {outliers:.2f}%")

# Plot histogram of outliers
plt.figure(figsize=(10, 5))
outlier_counts = outlier_mask.sum(axis=0)
plt.bar(df.columns, outlier_counts)
plt.xticks(rotation=45)
plt.title('Number of Outliers per Feature')
plt.show()

# Visualizing outliers with box plots for numerical columns
plt.figure(figsize=(12, 8))
for i, column in enumerate(df.columns, 1):
    plt.subplot(5, 5, i)
    sns.boxplot(x=df[column])
    plt.title(column)
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Drop the feature 'veil' from the DataFrame
df= df.drop(columns=['veil-type'])

# Calculate the correlation matrix for the filtered DataFrame
corr_matrix = df.corr()

# Plot heatmap of the correlation matrix
plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Features (without velli)')
plt.show()

"""Splitting Data"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

X = df.iloc[:, 1:]
y = df.iloc[:, 0]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"\nTraining feature:\n{X_train.head()}")
print(f"\nTraining target:\n{y_train.head()}")
print(f"\nTest features:\n{X_test.head()}")
print(f"\nTest target:\n{y_test.head()}")

print(f"\nTraining feature:\n{X_train.shape}")
print(f"\nTraining target:\n{y_train.shape}")
print(f"\nTest features:\n{X_test.shape}")
print(f"\nTest target:\n{y_test.shape}")

"""Implementing PCA"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Assuming X_train is your training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Fit PCA without specifying n_components
pca = PCA()
pca.fit(X_train_scaled)

# Calculate the cumulative explained variance
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

# Set the threshold for explained variance
threshold = 0.80

# Find the number of components that meet the threshold
best_n_components = np.argmax(cumulative_variance >= threshold) + 1  # +1 for correct count

# Print the best n_components
print(f"Best n_components to reach {threshold * 100}% explained variance: {best_n_components}")

# Plotting the cumulative explained variance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')
plt.title('Cumulative Explained Variance vs Number of Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid()
plt.axhline(y=threshold, color='r', linestyle='--', label='80% explained variance')
plt.axvline(x=best_n_components, color='g', linestyle='--', label=f'Best n_components: {best_n_components}')
plt.legend()
plt.show()

#IMPLIMENTING PCA with outliers
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

pca = PCA(n_components=9)  # Adjust the number of components based on variance explained
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Print explained variance ratio
print("Explained variance ratio:", pca.explained_variance_ratio_)

"""Implimenting LDA"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

# Apply LDA for dimensionality reduction
lda = LDA(n_components=1)
X_train_lda = lda.fit_transform(X_train_scaled, y_train)
X_test_lda = lda.transform(X_test_scaled)

# Perform LDA on the entire dataset
lda_full = LDA()
lda_full.fit(X_train, y_train)

# Predictions and evaluation
y_pred = lda_full.predict(X_test)
print("Accuracy with outliers:", accuracy_score(y_test, y_pred))

conf_mat=confusion_matrix(y_test, y_pred)
# Confusion matrix
sns.heatmap(conf_mat, annot=True, fmt="d", cmap="Blues")
plt.title("LDA Confusion Matrix on clean data ")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""Removing Outliers"""

# Remove outliers
df_clean = df_encoded[~outlier_mask.any(axis=1)]
df_clean = pd.DataFrame(df_clean, columns=[f'col_{i}' for i in range(23)])
X_clean = df_clean.iloc[:, 1:]
y_clean = df_clean.iloc[:, 0]

X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean, y_clean, test_size=0.3, random_state=42)

"""PCA without Outliers"""

#pca without outliers
scaler = StandardScaler()
X_train_scaled_clean = scaler.fit_transform(X_train_clean)
X_test_scaled_clean = scaler.transform(X_test_clean)

pca_clean = PCA(n_components=9)
X_train_pca_clean = pca_clean.fit_transform(X_train_scaled_clean)
X_test_pca_clean = pca_clean.transform(X_test_scaled_clean)

# Print explained variance ratio
print("Explained variance ratio:", pca_clean.explained_variance_ratio_)

"""LDA without Outliers"""

# Perform LDA on clean data without outliers
lda_clean = LDA()
lda_clean.fit(X_train_scaled_clean, y_train_clean)

# Predictions and evaluation
y_pred_clean = lda_clean.predict(X_test_scaled_clean)
print("Accuracy without outliers:", accuracy_score(y_test_clean, y_pred_clean))

cm=confusion_matrix(y_test_clean, y_pred_clean)
# Plot confusion matrix
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("LDA Confusion Matrix on Clean Data")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()


# Calculate the cumulative explained variance
cumulative_variance = np.cumsum(lda_clean.explained_variance_ratio_)

threshold = 0.95

# Find the number of components that meet the threshold
best_n_components = np.argmax(cumulative_variance >= threshold) + 1
# Print the best n_components
print(f"Best n_components to reach {threshold * 100}% explained variance: {best_n_components}")

"""# Classification on Original Dataset.

Cross validation for optimal K selection
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

k_values = range(1, 21)
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')
    cv_scores.append(scores.mean())

# Find the best k
best_k = k_values[cv_scores.index(max(cv_scores))]
print(f"The best k value is {best_k} with accuracy: {max(cv_scores) * 100:.2f}%")

# Plot
plt.plot(k_values, cv_scores)
plt.xlabel('Number of Neighbors K')
plt.ylabel('Cross-Validated Accuracy')
plt.title('Finding the Optimal K for KNN')
plt.show()

"""KNN"""

#KNN without pca with outliers
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

knn = KNeighborsClassifier(n_neighbors=1)

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("KNN Model Accuracy on Original Data:", accuracy)
print("KNN Classification Report on Original Data:\n", classification_report(y_test, y_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("KNN Confusion Matrix on Original Data")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""Logistic Regression"""

#logisticregression with outliers
from sklearn.linear_model import LogisticRegression
log_reg_with_outliers = LogisticRegression(max_iter=1000, random_state=42)
log_reg_with_outliers.fit(X_train, y_train)

# Predictions and evaluation
y_pred_with_outliers = log_reg_with_outliers.predict(X_test)
accuracy_with_outliers = accuracy_score(y_test, y_pred_with_outliers)
print(f"Accuracy with outliers: {accuracy_with_outliers:.4f}")
print("Classification Report on Original Data:\n", classification_report(y_test, y_pred))

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, y_pred_with_outliers), annot=True, fmt='d', cmap='Blues',
            xticklabels=['edible', 'poisonous'], yticklabels=['edible', 'poisonous'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Logistic Regression (with outliers)')
plt.show()

"""SVM"""

#SVM without PCA with outliers
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

model_no_pca = SVC(kernel='linear')
model_no_pca.fit(X_train_scaled, y_train)
y_pred_no_pca = model_no_pca.predict(X_test_scaled)

accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)
print(f"Accuracy without PCA and with outliers: {accuracy_no_pca}")
print("Classification Report on Original Data:\n", classification_report(y_test, y_pred))
# Calculate confusion matrix for the model without PCA
conf_matrix_no_pca = confusion_matrix(y_test, y_pred_no_pca)

# Plot confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_no_pca, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM without PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

#SVM without PCA with outliers
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

model_no_pca = SVC(kernel='poly')
model_no_pca.fit(X_train_scaled, y_train)
y_pred_no_pca = model_no_pca.predict(X_test_scaled)

accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)
print(f"Accuracy without PCA and with outliers: {accuracy_no_pca}")
print("Classification Report on Original Data:\n", classification_report(y_test, y_pred))
# Calculate confusion matrix for the model without PCA
conf_matrix_no_pca = confusion_matrix(y_test, y_pred_no_pca)

# Plot confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_no_pca, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM without PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

#SVM without PCA with outliers
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

model_no_pca = SVC(kernel='rbf')
model_no_pca.fit(X_train_scaled, y_train)
y_pred_no_pca = model_no_pca.predict(X_test_scaled)

accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)
print(f"Accuracy without PCA and with outliers: {accuracy_no_pca}")
print("Classification Report on Original Data:\n", classification_report(y_test, y_pred))
# Calculate confusion matrix for the model without PCA
conf_matrix_no_pca = confusion_matrix(y_test, y_pred_no_pca)

# Plot confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_no_pca, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM without PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

"""Decision Tree using gini index"""

#with gini
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import graphviz

clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)

clf_gini.fit(X_train, y_train)

y_pred_train_gini = clf_gini.predict(X_train)
y_pred_gini = clf_gini.predict(X_test)


print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_gini)*100))
print('Test-set accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_gini)*100))
features = X.columns.tolist()
classes = ['edible', 'poisonous']
dot_data = tree.export_graphviz(clf_gini, out_file=None,
                              feature_names=X.columns,
                              class_names=classes,
                              filled=True, rounded=True,
                              special_characters=True)

graph = graphviz.Source(dot_data)

graph

# Print the Confusion Matrix
cm = confusion_matrix(y_test, y_pred_gini)

cm_df = pd.DataFrame(cm, index=classes, columns=classes)

plt.figure(figsize=(10,10))
sns.heatmap(cm_df, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r')
plt.ylabel('Actual label');
plt.xlabel('Predicted label');

"""Decision Tree using entropy"""

#Using entropy
clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)

clf_en.fit(X_train, y_train)

y_pred_train_en = clf_en.predict(X_train)
y_pred_en = clf_en.predict(X_test)

from sklearn.metrics import accuracy_score

print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_en)*100))
print('Test-set accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_en)*100))

dot_data = tree.export_graphviz(clf_en, out_file=None,
                              feature_names=X_train.columns,
                              class_names=classes,
                              filled=True, rounded=True,
                              special_characters=True)

graph = graphviz.Source(dot_data)

graph

# Print the Confusion Matrix
cm = confusion_matrix(y_test, y_pred_en)

cm_df = pd.DataFrame(cm, index=classes, columns=classes)
plt.figure(figsize=(10,10))
sns.heatmap(cm_df, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r')
plt.ylabel('Actual label');
plt.xlabel('Predicted label');

"""# Classification of data using PCA

## With PCA and With Outliers

KNN
"""

# KNN WITH PCA
knn_pca = KNeighborsClassifier(n_neighbors=1)
knn_pca.fit(X_train_pca, y_train)
# Predict on PCA-transformed test set
y_knn_pred_pca = knn_pca.predict(X_test_pca)

accuracy_knn_pca = accuracy_score(y_test, y_knn_pred_pca)
print("KNN Model Accuracy on PCA Data:", accuracy_knn_pca)
print("KNN Classification Report on PCA Data:\n", classification_report(y_test, y_knn_pred_pca))

cm_pca = confusion_matrix(y_test, y_knn_pred_pca)
sns.heatmap(cm_pca, annot=True, fmt="d", cmap="Oranges")
plt.title("KNN Confusion Matrix on PCA Data")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""Logistic Regression"""

#L.R WITH PCA
lr_train_pca = pca.fit_transform(X_train)
lr_test_pca = pca.transform(X_test)

# Train logistic regression
lr_with_outliers = LogisticRegression(max_iter=1000, random_state=42)
lr_with_outliers.fit(X_train_pca, y_train)

# Predictions and evaluation
y_lr_pred_with_outliers = lr_with_outliers.predict(X_test_pca)
accuracy_lr_with_outliers = accuracy_score(y_test, y_lr_pred_with_outliers)
print(f"Accuracy with outliers (PCA): {accuracy_lr_with_outliers:.4f}")

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, y_lr_pred_with_outliers), annot=True, fmt='d', cmap='Blues',
            xticklabels=['edible', 'poisonous'], yticklabels=['edible', 'poisonous'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Logistic Regression with PCA (with outliers)')
plt.show()

"""SVM"""

#SVM with PCA
model_svm_pca = SVC()
model_svm_pca.fit(X_train_pca, y_train)
y_svm_pred_pca = model_svm_pca.predict(X_test_pca)

accuracy_svm_pca = accuracy_score(y_test, y_svm_pred_pca)
print(f"Accuracy with PCA and outliers: {accuracy_svm_pca}")

conf_svm_matrix = confusion_matrix(y_test, y_svm_pred_pca)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_svm_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM with PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

#SVM with PCA
model_svm_pca = SVC(kernel='linear')
model_svm_pca.fit(X_train_pca, y_train)
y_svm_pred_pca = model_svm_pca.predict(X_test_pca)

accuracy_svm_pca = accuracy_score(y_test, y_svm_pred_pca)
print(f"Accuracy with PCA and outliers: {accuracy_svm_pca}")

conf_svm_matrix = confusion_matrix(y_test, y_svm_pred_pca)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_svm_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM with PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

#SVM with PCA
model_svm_pca = SVC(kernel='poly')
model_svm_pca.fit(X_train_pca, y_train)
y_svm_pred_pca = model_svm_pca.predict(X_test_pca)

accuracy_svm_pca = accuracy_score(y_test, y_svm_pred_pca)
print(f"Accuracy with PCA and outliers: {accuracy_svm_pca}")

conf_svm_matrix = confusion_matrix(y_test, y_svm_pred_pca)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_svm_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM with PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

#SVM with PCA
model_svm_pca = SVC(kernel='rbf')
model_svm_pca.fit(X_train_pca, y_train)
y_svm_pred_pca = model_svm_pca.predict(X_test_pca)

accuracy_svm_pca = accuracy_score(y_test, y_svm_pred_pca)
print(f"Accuracy with PCA and outliers: {accuracy_svm_pca}")

conf_svm_matrix = confusion_matrix(y_test, y_svm_pred_pca)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_svm_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM with PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

"""Decision Tree"""

#D.T with PCA
X_dt_train_pca = pca.fit_transform(X_train_scaled)
X_dt_test_pca = pca.transform(X_test_scaled)

# Train Decision Tree model
model = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)
model.fit(X_dt_train_pca, y_train)

y_dt_pred = model.predict(X_test_pca)


print(classification_report(y_test, y_dt_pred))

accuracy = accuracy_score(y_test, y_dt_pred)
print(f'Accuracy: {accuracy:.4f}')

conf_matrix = confusion_matrix(y_test, y_dt_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['edible', 'poisonous'], yticklabels=['edible', 'poisonous'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix with Outliers')
plt.show()

"""## with PCA without Outliers

KNN
"""

#KNN with PCA without outliers
knn_pca_clean = KNeighborsClassifier(n_neighbors=1)
knn_pca_clean.fit(X_train_pca_clean, y_train_clean)

# Predict on PCA-transformed test set
y_knn_pred_pca_clean = knn_pca_clean.predict(X_test_pca_clean)

accuracy_knn_pca_clean = accuracy_score(y_test_clean, y_knn_pred_pca_clean)
print("KNN Model Accuracy on PCA Data:", accuracy_knn_pca_clean)
print("KNN Classification Report on PCA Data:\n", classification_report(y_test_clean, y_knn_pred_pca_clean))

cm_pca = confusion_matrix(y_test_clean, y_knn_pred_pca_clean)
sns.heatmap(cm_pca, annot=True, fmt="d", cmap="Oranges")
plt.title("KNN Confusion Matrix on PCA Data")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""Logistic Regression"""

#L.R with pca without outliers
lr_train_pca_clean = pca.fit_transform(X_train_clean)
lr_test_pca_clean = pca.transform(X_test_clean)

# Train logistic regression
lr_clean = LogisticRegression(max_iter=1000, random_state=42)
lr_clean.fit(X_train_pca_clean, y_train_clean)

y_lr_pred_clean = lr_clean.predict(X_test_pca_clean)
accuracy_lr_clean = accuracy_score(y_test_clean, y_lr_pred_clean)
print(f"Accuracy with outliers (PCA): {accuracy_lr_clean:.4f}")

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test_clean, y_lr_pred_clean), annot=True, fmt='d', cmap='Blues',
            xticklabels=['edible', 'poisonous'], yticklabels=['edible', 'poisonous'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Logistic Regression with PCA (with outliers)')
plt.show()

"""SVM"""

#SVM with PCA without outliers
svm_pca_clean = SVC()
svm_pca_clean.fit(X_train_pca_clean, y_train_clean)
y_svm_pred_pca_clean = svm_pca_clean.predict(X_test_pca_clean)

accuracy_svm_pca_clean = accuracy_score(y_test_clean, y_svm_pred_pca_clean)
print(f"Accuracy with PCA and outliers: {accuracy_svm_pca_clean}")

conf_svm_matrix = confusion_matrix(y_test_clean, y_svm_pred_pca_clean)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_svm_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM with PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

#SVM with PCA without outliers
svm_pca_clean = SVC(kernel='linear')
svm_pca_clean.fit(X_train_pca_clean, y_train_clean)
y_svm_pred_pca_clean = svm_pca_clean.predict(X_test_pca_clean)

accuracy_svm_pca_clean = accuracy_score(y_test_clean, y_svm_pred_pca_clean)
print(f"Accuracy with PCA and outliers: {accuracy_svm_pca_clean}")

conf_svm_matrix = confusion_matrix(y_test_clean, y_svm_pred_pca_clean)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_svm_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM with PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

#SVM with PCA without outliers
svm_pca_clean = SVC(kernel='poly')
svm_pca_clean.fit(X_train_pca_clean, y_train_clean)
y_svm_pred_pca_clean = svm_pca_clean.predict(X_test_pca_clean)

accuracy_svm_pca_clean = accuracy_score(y_test_clean, y_svm_pred_pca_clean)
print(f"Accuracy with PCA and outliers: {accuracy_svm_pca_clean}")

conf_svm_matrix = confusion_matrix(y_test_clean, y_svm_pred_pca_clean)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_svm_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM with PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

#SVM with PCA without outliers
svm_pca_clean = SVC(kernel='rbf')
svm_pca_clean.fit(X_train_pca_clean, y_train_clean)
y_svm_pred_pca_clean = svm_pca_clean.predict(X_test_pca_clean)

accuracy_svm_pca_clean = accuracy_score(y_test_clean, y_svm_pred_pca_clean)
print(f"Accuracy with PCA and outliers: {accuracy_svm_pca_clean}")

conf_svm_matrix = confusion_matrix(y_test_clean, y_svm_pred_pca_clean)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_svm_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for SVM with PCA')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

"""Decision Tree"""

#D.T with PCA without outliers
X_dt_train_pca_clean = pca.fit_transform(X_train_scaled_clean)
X_dt_test_pca_clean = pca.transform(X_test_scaled_clean)

# Train Decision Tree model
model = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)
model.fit(X_dt_train_pca_clean, y_train_clean)

y_dt_pred_clean = model.predict(X_dt_test_pca_clean)

print(classification_report(y_test_clean, y_dt_pred_clean))

accuracy_dt_clean = accuracy_score(y_test_clean, y_dt_pred_clean)
print(f'Accuracy: {accuracy_dt_clean:.4f}')

conf_matrix_clean = confusion_matrix(y_test_clean, y_dt_pred_clean)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['edible', 'poisonous'], yticklabels=['edible', 'poisonous'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix with Outliers')
plt.show()

"""# Classification of data with LDA

## with LDA with Outliers

KNN
"""

# Train the KNN model on the LDA-transformed data (optional)
knn_lda = KNeighborsClassifier(n_neighbors=1)
knn_lda.fit(X_train_lda, y_train)
# Predict on LDA-transformed test set
y_pred_lda = knn_lda.predict(X_test_lda)

accuracy_lda = accuracy_score(y_test, y_pred_lda)
print("KNN Model Accuracy on LDA Data:", accuracy_lda)
print("KNN Classification Report on LDA Data:\n", classification_report(y_test, y_pred_lda))

cm_lda = confusion_matrix(y_test, y_pred_lda)
sns.heatmap(cm_lda, annot=True, fmt="d", cmap="Greens")
plt.title("KNN Confusion Matrix on LDA Data")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""Logistic Rigression"""

# Check the number of unique classes in y_train
n_classes = len(np.unique(y_train))

n_components = min(n_classes - 1, X_train_scaled.shape[1])
# n_components should be at most (number of classes - 1) and less than or equal to the number of features

# Apply LDA
lda = LDA(n_components=n_components)
lr_train_lda = lda.fit_transform(X_train_scaled, y_train)
lr_test_lda = lda.transform(X_test_scaled)

# Train a classifier on the LDA-transformed training data
classifier = LogisticRegression()
classifier.fit(lr_train_lda, y_train)

y_lr_pred = classifier.predict(X_test_lda)

accuracy_lr = accuracy_score(y_test, y_lr_pred)
print("Accuracy:", accuracy_lr)

conf_matrix = confusion_matrix(y_test, y_lr_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and SVM (with outliers)')
plt.show()

"""SVM"""

# Train an SVM classifier on the LDA-transformed training data
svm = SVC()
svm.fit(X_train_lda, y_train)

# Predictions and evaluation for SVM with outliers
y_pred = svm.predict(X_test_lda)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy with LDA and SVM (with outliers):", accuracy)

conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and SVM (with outliers)')
plt.show()

# Train an SVM classifier on the LDA-transformed training data
svm = SVC(kernel='linear')
svm.fit(X_train_lda, y_train)

# Predictions and evaluation for SVM with outliers
y_pred = svm.predict(X_test_lda)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy with LDA and SVM (with outliers):", accuracy)

conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and SVM (with outliers)')
plt.show()

# Train an SVM classifier on the LDA-transformed training data
svm = SVC(kernel='poly')
svm.fit(X_train_lda, y_train)

# Predictions and evaluation for SVM with outliers
y_pred = svm.predict(X_test_lda)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy with LDA and SVM (with outliers):", accuracy)

conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and SVM (with outliers)')
plt.show()

# Train an SVM classifier on the LDA-transformed training data
svm = SVC(kernel='rbf')
svm.fit(X_train_lda, y_train)

# Predictions and evaluation for SVM with outliers
y_pred = svm.predict(X_test_lda)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy with LDA and SVM (with outliers):", accuracy)

conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and SVM (with outliers)')
plt.show()

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

# Check and adjust lengths of the training and test data
X_train_lda_shape, y_train_shape = X_train_lda.shape[0], len(y_train)
if X_train_lda_shape != y_train_shape:
    y_train = y_train[:X_train_lda_shape]

X_test_lda_shape, y_test_shape = X_test_lda.shape[0], len(y_test)
if X_test_lda_shape != y_test_shape:
    y_test = y_test[:X_test_lda_shape]

# Train a Decision Tree classifier on the LDA-transformed training data
tree = DecisionTreeClassifier()
tree.fit(X_train_lda, y_train)

y_pred = tree.predict(X_test_lda)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy with LDA and Decision Tree (with outliers):", accuracy)

conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and Decision Tree (with outliers)')
plt.show()

"""## with LDA without Outliers

KNN
"""

# Apply LDA to your cleaned training data
lda = LDA(n_components=1) # Initialize LDA
knn_train_lda_clean = lda.fit_transform(X_train_clean, y_train_clean)
knn_test_lda_clean = lda.transform(X_test_clean)

# Train a KNN classifier on the LDA-transformed training data
knn_clean = KNeighborsClassifier(n_neighbors=1)
knn.fit(knn_train_lda_clean, y_train_clean)

y_pred_clean_knn = knn.predict(knn_test_lda_clean)

accuracy_clean_knn = accuracy_score(y_test_clean, y_pred_clean_knn)
print("Accuracy with LDA and KNN (clean data):", accuracy_clean_knn)

cm_knn = confusion_matrix(y_test_clean, y_pred_clean_knn)
sns.heatmap(cm_knn, annot=True, fmt="d", cmap="Greens")
plt.title("KNN Confusion Matrix on LDA Data")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""Logistic Regression"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_clean)
X_test_scaled = scaler.transform(X_test_clean)

n_classes = len(np.unique(y_train_clean))

# Adjust n_components based on the number of features and classes
n_components = min(n_classes - 1, X_train_scaled.shape[1])

# Apply LDA
lda = LDA(n_components=n_components)
X_train_lda = lda.fit_transform(X_train_scaled, y_train_clean)
X_test_lda = lda.transform(X_test_scaled)

# Train a classifier on the LDA-transformed training data
classifier = LogisticRegression()
classifier.fit(X_train_lda, y_train_clean)

y_pred_clean_lr = classifier.predict(X_test_lda)

accuracy_clean = accuracy_score(y_test_clean, y_pred_clean_lr)
print("Accuracy:", accuracy_clean)

conf_matrix_clean = confusion_matrix(y_test_clean, y_pred_clean_lr)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_clean, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and SVM (with outliers)')
plt.show()

"""SVM"""

# Train an SVM classifier on the LDA-transformed clean training data
svm_clean = SVC()
svm_clean.fit(X_train_scaled_clean, y_train_clean)

y_pred_clean_svm = svm_clean.predict(X_test_scaled_clean)

accuracy_clean_svm = accuracy_score(y_test_clean, y_pred_clean_svm)
print("Accuracy with LDA and SVM (without outliers):", accuracy_clean_svm)

conf_matrix_clean_svm = confusion_matrix(y_test_clean, y_pred_clean_svm)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_clean_svm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and SVM (without outliers)')
plt.show()

# Train an SVM classifier on the LDA-transformed clean training data
svm_clean = SVC(kernel='linear')
svm_clean.fit(X_train_scaled_clean, y_train_clean)

y_pred_clean_svm = svm_clean.predict(X_test_scaled_clean)

accuracy_clean_svm = accuracy_score(y_test_clean, y_pred_clean_svm)
print("Accuracy with LDA and SVM (without outliers):", accuracy_clean_svm)

conf_matrix_clean_svm = confusion_matrix(y_test_clean, y_pred_clean_svm)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_clean_svm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and SVM (without outliers)')
plt.show()

# Train an SVM classifier on the LDA-transformed clean training data
svm_clean = SVC(kernel='poly')
svm_clean.fit(X_train_scaled_clean, y_train_clean)

y_pred_clean_svm = svm_clean.predict(X_test_scaled_clean)

accuracy_clean_svm = accuracy_score(y_test_clean, y_pred_clean_svm)
print("Accuracy with LDA and SVM (without outliers):", accuracy_clean_svm)

conf_matrix_clean_svm = confusion_matrix(y_test_clean, y_pred_clean_svm)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_clean_svm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and SVM (without outliers)')
plt.show()

# Train an SVM classifier on the LDA-transformed clean training data
svm_clean = SVC(kernel='rbf')
svm_clean.fit(X_train_scaled_clean, y_train_clean)

y_pred_clean_svm = svm_clean.predict(X_test_scaled_clean)

accuracy_clean_svm = accuracy_score(y_test_clean, y_pred_clean_svm)
print("Accuracy with LDA and SVM (without outliers):", accuracy_clean_svm)

conf_matrix_clean_svm = confusion_matrix(y_test_clean, y_pred_clean_svm)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_clean_svm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and SVM (without outliers)')
plt.show()

"""Decision Tree"""

# Train a Decision Tree classifier on the LDA-transformed clean training data
tree_clean = DecisionTreeClassifier()
tree_clean.fit(X_train_scaled_clean, y_train_clean)

y_pred_clean = tree_clean.predict(X_test_scaled_clean)

accuracy_clean = accuracy_score(y_test_clean, y_pred_clean)
print("Accuracy with LDA and Decision Tree (without outliers):", accuracy_clean)

conf_matrix_clean = confusion_matrix(y_test_clean, y_pred_clean)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_clean, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix with LDA and Decision Tree (without outliers)')
plt.show()